{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmRrJh4Dhan3",
        "outputId": "32a68e84-260c-418d-ee9e-3140c117f695"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dR11E8shhakN",
        "outputId": "a445a83f-1893-4195-8d2a-50ebaa5c7a67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import random\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/paper/archive (3).zip'\n",
        "extract_path = '/content/drive/MyDrive/data'\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "base_dir = '/content/retinopathy_split'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "val_dir = os.path.join(base_dir, 'val')\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "\n",
        "for directory in [train_dir, val_dir, test_dir]:\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "classes = ['0', '1', '2', '3', '4']\n",
        "\n",
        "for c in classes:\n",
        "    for split in [train_dir, val_dir, test_dir]:\n",
        "        os.makedirs(os.path.join(split, c), exist_ok=True)\n",
        "\n",
        "for c in classes:\n",
        "    class_dir = os.path.join(extract_path, c)\n",
        "    images = os.listdir(class_dir)\n",
        "    random.shuffle(images)\n",
        "\n",
        "    train_split = int(0.7 * len(images))\n",
        "    val_split = int(0.85 * len(images))\n",
        "\n",
        "    train_images = images[:train_split]\n",
        "    val_images = images[train_split:val_split]\n",
        "    test_images = images[val_split:]\n",
        "\n",
        "    # Image translation\n",
        "    for img in train_images:\n",
        "        shutil.copy(os.path.join(class_dir, img), os.path.join(train_dir, c, img))\n",
        "    for img in val_images:\n",
        "        shutil.copy(os.path.join(class_dir, img), os.path.join(val_dir, c, img))\n",
        "    for img in test_images:\n",
        "        shutil.copy(os.path.join(class_dir, img), os.path.join(test_dir, c, img))\n",
        "\n",
        "print(\"The dataset was successfully divided‚úÖ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0OoUGIuhahm",
        "outputId": "61c57932-4311-4dc5-98d9-b3d8b2a24bb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dataset was successfully divided‚úÖ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# ‚ö°Ô∏è Training-specific augmentations\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# üå± Transformations for validation and testing (without augmentations)\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# üìÇ Data loading\n",
        "train_data = datasets.ImageFolder(root='/content/retinopathy_split/train', transform=train_transform)\n",
        "val_data = datasets.ImageFolder(root='/content/retinopathy_split/val', transform=eval_transform)\n",
        "test_data = datasets.ImageFolder(root='/content/retinopathy_split/test', transform=eval_transform)\n",
        "\n",
        "# üß∫ Data Loaders\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_data, batch_size=32, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_data, batch_size=32, shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "id": "RLGlbjvUhafC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "# Loading the MobileNetV2 model pretrained on ImageNet\n",
        "mobilenet = models.mobilenet_v2(pretrained=True)\n",
        "\n",
        "# Adding a higher Dropout rate\n",
        "mobilenet.classifier[0] = nn.Dropout(p=0.5)\n",
        "mobilenet.classifier[1] = nn.Linear(mobilenet.classifier[1].in_features, 5)\n",
        "\n",
        "# Transfer the model to the GPU if it is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "mobilenet = mobilenet.to(device)\n",
        "\n",
        "# Loss Function and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(mobilenet.parameters(), lr=1e-4)\n",
        "\n",
        "# Learning rate decay during training\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSrNl-oshadA",
        "outputId": "2e11f041-0c43-4ac2-b5a9-ca12053af4ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13.6M/13.6M [00:00<00:00, 125MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    mobilenet.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = mobilenet(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_acc = 100 * correct / total\n",
        "\n",
        "    val_loss, val_acc = evaluate(mobilenet, val_loader)\n",
        "    test_loss, test_acc = evaluate(mobilenet, test_loader)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "    print(f\"  Train     => Loss: {train_loss:.4f}, Accuracy: {train_acc:.2f}%\")\n",
        "    print(f\"  Validation=> Loss: {val_loss:.4f}, Accuracy: {val_acc:.2f}%\")\n",
        "    print(f\"  Test      => Loss: {test_loss:.4f}, Accuracy: {test_acc:.2f}%\")\n",
        "    print(\"-\" * 60)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrGTlVgThaao",
        "outputId": "be37eaee-324f-4330-918c-3e67059a898e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20]\n",
            "  Train     => Loss: 0.7976, Accuracy: 73.41%\n",
            "  Validation=> Loss: 0.6947, Accuracy: 76.14%\n",
            "  Test      => Loss: 0.6965, Accuracy: 76.74%\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "param_grid = {\n",
        "    'batch_size': [16, 32],\n",
        "    'lr': [1e-3, 1e-4],\n",
        "    'dropout': [0.3, 0.5]\n",
        "}\n",
        "\n",
        "grid = list(ParameterGrid(param_grid))\n"
      ],
      "metadata": {
        "id": "FgHKiAAmhaY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-gradcam"
      ],
      "metadata": {
        "id": "5tgVvnTrhaUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_m-431WdkDw"
      },
      "outputs": [],
      "source": [
        "pip install captum"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision captum matplotlib numpy\n",
        "\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from captum.attr import IntegratedGradients\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
        "model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, 5)\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_dataset = datasets.ImageFolder(\n",
        "    '/content/retinopathy_split/test',\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "def show_image(img_tensor):\n",
        "    img = img_tensor.cpu().numpy().transpose(1, 2, 0)\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    img = std * img + mean\n",
        "    img = np.clip(img, 0, 1)\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "def visualize_attr(attr):\n",
        "    attr = attr.cpu().detach().numpy()\n",
        "\n",
        "    if attr.ndim == 4:\n",
        "        attr = attr.squeeze(0)\n",
        "    if attr.ndim == 3 and attr.shape[0] == 1:\n",
        "        attr = attr.squeeze(0)\n",
        "    elif attr.ndim == 3 and attr.shape[0] == 3:\n",
        "        attr = attr.transpose(1, 2, 0)\n",
        "        attr = np.sum(np.abs(attr), axis=2)\n",
        "    elif attr.ndim == 2:\n",
        "        pass\n",
        "    else:\n",
        "        raise ValueError(f\"ÿ¥ŸÉŸÑ ÿ∫Ÿäÿ± ŸÖÿ™ŸàŸÇÿπ ŸÑŸÑÿ®ŸäÿßŸÜÿßÿ™: {attr.shape}\")\n",
        "\n",
        "    attr = (attr - attr.min()) / (attr.max() - attr.min() + 1e-8)\n",
        "\n",
        "    plt.imshow(attr, cmap='hot')\n",
        "    plt.colorbar()\n",
        "    plt.axis('off')\n",
        "    plt.title('Importance Map')\n",
        "    plt.show()\n",
        "\n",
        "idx = random.randint(0, len(test_dataset)-1)\n",
        "image, label = test_dataset[idx]\n",
        "image = image.to(device)\n",
        "label = torch.tensor(label).to(device)\n",
        "\n",
        "ig = IntegratedGradients(model)\n",
        "attributions, delta = ig.attribute(\n",
        "    inputs=image.unsqueeze(0),\n",
        "    target=label,\n",
        "    return_convergence_delta=True\n",
        ")\n",
        "\n",
        "print(f\"Convergence Delta: {delta.item():.4f}\")\n",
        "show_image(image)\n",
        "visualize_attr(attributions)\n"
      ],
      "metadata": {
        "id": "4dLRk7p2iFKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ÿ´ÿßŸÜŸäÿßŸã: ÿßÿ≥ÿ™Ÿäÿ±ÿßÿØ ÿßŸÑŸÖŸÉÿ™ÿ®ÿßÿ™\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from captum.attr import IntegratedGradients\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# ÿ´ÿßŸÑÿ´ÿßŸã: ÿ™ÿπÿ±ŸäŸÅ ÿßŸÑÿ¨Ÿáÿßÿ≤ (GPU ÿ•ÿ∞ÿß ŸÖÿ™ŸàŸÅÿ±)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ÿ±ÿßÿ®ÿπÿßŸã: ÿ™ÿ≠ŸÖŸäŸÑ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ÿßŸÑŸÖÿØÿ±ÿ® (Ÿäÿ¨ÿ® ÿßÿ≥ÿ™ÿ®ÿØÿßŸÑ Ÿáÿ∞ÿß ÿßŸÑÿ¨ÿ≤ÿ° ÿ®ŸÜŸÖŸàÿ∞ÿ¨ŸÉ ÿßŸÑÿÆÿßÿµ)\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
        "model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, 5)  # ÿ™ÿπÿØŸäŸÑ ŸÑ 5 ŸÅÿ¶ÿßÿ™\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# ÿÆÿßŸÖÿ≥ÿßŸã: ÿ•ÿπÿØÿßÿØ ÿ™ÿ≠ŸàŸäŸÑÿßÿ™ ÿßŸÑÿµŸàÿ±\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# ÿ≥ÿßÿØÿ≥ÿßŸã: ÿ™ÿ≠ŸÖŸäŸÑ ÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑÿßÿÆÿ™ÿ®ÿßÿ±\n",
        "test_dataset = datasets.ImageFolder(\n",
        "    '/content/retinopathy_split/test',  # ÿ™ÿπÿØŸäŸÑ ÿßŸÑŸÖÿ≥ÿßÿ± ÿ≠ÿ≥ÿ® ÿ®ŸäÿßŸÜÿßÿ™ŸÉ\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "# ÿ≥ÿßÿ®ÿπÿßŸã: ÿØÿßŸÑÿ© ŸÑÿπÿ±ÿ∂ ÿßŸÑÿµŸàÿ±ÿ© ÿßŸÑÿ£ÿµŸÑŸäÿ©\n",
        "def show_image(img_tensor):\n",
        "    img = img_tensor.cpu().numpy().transpose(1, 2, 0)\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    img = std * img + mean  # ÿπŸÉÿ≥ ÿßŸÑÿ™ÿ∑ÿ®Ÿäÿπ\n",
        "    img = np.clip(img, 0, 1)\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# ÿ´ÿßŸÖŸÜÿßŸã: ÿØÿßŸÑÿ© ŸÖÿ±ŸÜÿ© ŸÑÿπÿ±ÿ∂ ÿÆÿ±Ÿäÿ∑ÿ© ÿßŸÑÿ£ŸáŸÖŸäÿ©\n",
        "def visualize_attr(attr):\n",
        "    attr = attr.cpu().detach().numpy()\n",
        "\n",
        "    # ŸÖÿπÿßŸÑÿ¨ÿ© ÿßŸÑÿ£ÿ®ÿπÿßÿØ ÿßŸÑŸÖÿÆÿ™ŸÑŸÅÿ©\n",
        "    if attr.ndim == 4:\n",
        "        attr = attr.squeeze(0)\n",
        "    if attr.ndim == 3 and attr.shape[0] == 1:\n",
        "        attr = attr.squeeze(0)\n",
        "    elif attr.ndim == 3 and attr.shape[0] == 3:\n",
        "        attr = attr.transpose(1, 2, 0)\n",
        "        attr = np.sum(np.abs(attr), axis=2)\n",
        "    elif attr.ndim == 2:\n",
        "        pass\n",
        "    else:\n",
        "        raise ValueError(f\"ÿ¥ŸÉŸÑ ÿ∫Ÿäÿ± ŸÖÿ™ŸàŸÇÿπ ŸÑŸÑÿ®ŸäÿßŸÜÿßÿ™: {attr.shape}\")\n",
        "\n",
        "    # ÿ™ÿ∑ÿ®Ÿäÿπ ÿßŸÑŸÇŸäŸÖ\n",
        "    attr = (attr - attr.min()) / (attr.max() - attr.min() + 1e-8)\n",
        "\n",
        "    # ÿßŸÑÿπÿ±ÿ∂\n",
        "    plt.imshow(attr, cmap='hot')\n",
        "    plt.colorbar()\n",
        "    plt.axis('off')\n",
        "    plt.title('ÿÆÿ±Ÿäÿ∑ÿ© ÿßŸÑÿ£ŸáŸÖŸäÿ©')\n",
        "    plt.show()\n",
        "\n",
        "# ÿ™ÿßÿ≥ÿπÿßŸã: ÿßÿÆÿ™Ÿäÿßÿ± ÿπŸäŸÜÿ© ÿπÿ¥Ÿàÿßÿ¶Ÿäÿ© ŸÖŸÜ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™\n",
        "idx = random.randint(0, len(test_dataset)-1)\n",
        "image, label = test_dataset[idx]\n",
        "image = image.to(device)\n",
        "label = torch.tensor(label).to(device)\n",
        "\n",
        "# ÿπÿßÿ¥ÿ±ÿßŸã: ÿ≠ÿ≥ÿßÿ® ÿÆÿ±Ÿäÿ∑ÿ© ÿßŸÑÿ£ŸáŸÖŸäÿ©\n",
        "ig = IntegratedGradients(model)\n",
        "attributions, delta = ig.attribute(\n",
        "    inputs=image.unsqueeze(0),\n",
        "    target=label,\n",
        "    return_convergence_delta=True\n",
        ")\n",
        "\n",
        "# ÿßŸÑÿ≠ÿßÿØŸä ÿπÿ¥ÿ±: ÿπÿ±ÿ∂ ÿßŸÑŸÜÿ™ÿßÿ¶ÿ¨\n",
        "print(f\"Convergence Delta: {delta.item():.4f}\")\n",
        "show_image(image)\n",
        "visualize_attr(attributions)\n"
      ],
      "metadata": {
        "id": "6l9yCq1wiFG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # ÿ≠ÿ≥ÿßÿ® ÿßŸÑŸÖŸÇÿßŸäŸäÿ≥\n",
        "    print(classification_report(all_labels, all_preds, target_names=class_names, digits=4))\n",
        "\n",
        "    # ÿ∑ÿ®ÿßÿπÿ© confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "# ÿßÿ≥ÿ™ÿØÿπÿßÿ° ÿßŸÑÿØÿßŸÑÿ© ÿπŸÑŸâ test_loader\n",
        "evaluate_model(mobilenet, test_loader)"
      ],
      "metadata": {
        "id": "YWJMxDVwR5ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://drive.google.com/file/d/1LD6v33J7z5Vt4CnVOAs4kWICHIt5JLyg/view?usp=sharing"
      ],
      "metadata": {
        "id": "n0gwdTC9X0IB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}