{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmRrJh4Dhan3",
        "outputId": "32a68e84-260c-418d-ee9e-3140c117f695"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dR11E8shhakN",
        "outputId": "a445a83f-1893-4195-8d2a-50ebaa5c7a67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import random\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/paper/archive (3).zip'\n",
        "extract_path = '/content/drive/MyDrive/data'\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "base_dir = '/content/retinopathy_split'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "val_dir = os.path.join(base_dir, 'val')\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "\n",
        "for directory in [train_dir, val_dir, test_dir]:\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "classes = ['0', '1', '2', '3', '4']\n",
        "\n",
        "for c in classes:\n",
        "    for split in [train_dir, val_dir, test_dir]:\n",
        "        os.makedirs(os.path.join(split, c), exist_ok=True)\n",
        "\n",
        "for c in classes:\n",
        "    class_dir = os.path.join(extract_path, c)\n",
        "    images = os.listdir(class_dir)\n",
        "    random.shuffle(images)\n",
        "\n",
        "    train_split = int(0.7 * len(images))\n",
        "    val_split = int(0.85 * len(images))\n",
        "\n",
        "    train_images = images[:train_split]\n",
        "    val_images = images[train_split:val_split]\n",
        "    test_images = images[val_split:]\n",
        "\n",
        "    # Image translation\n",
        "    for img in train_images:\n",
        "        shutil.copy(os.path.join(class_dir, img), os.path.join(train_dir, c, img))\n",
        "    for img in val_images:\n",
        "        shutil.copy(os.path.join(class_dir, img), os.path.join(val_dir, c, img))\n",
        "    for img in test_images:\n",
        "        shutil.copy(os.path.join(class_dir, img), os.path.join(test_dir, c, img))\n",
        "\n",
        "print(\"The dataset was successfully dividedâœ…\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0OoUGIuhahm",
        "outputId": "61c57932-4311-4dc5-98d9-b3d8b2a24bb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dataset was successfully dividedâœ…\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# âš¡ï¸ Training-specific augmentations\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# ğŸŒ± Transformations for validation and testing (without augmentations)\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# ğŸ“‚ Data loading\n",
        "train_data = datasets.ImageFolder(root='/content/retinopathy_split/train', transform=train_transform)\n",
        "val_data = datasets.ImageFolder(root='/content/retinopathy_split/val', transform=eval_transform)\n",
        "test_data = datasets.ImageFolder(root='/content/retinopathy_split/test', transform=eval_transform)\n",
        "\n",
        "# ğŸ§º Data Loaders\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_data, batch_size=32, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_data, batch_size=32, shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "id": "RLGlbjvUhafC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "# Loading the MobileNetV2 model pretrained on ImageNet\n",
        "mobilenet = models.mobilenet_v2(pretrained=True)\n",
        "\n",
        "# Adding a higher Dropout rate\n",
        "mobilenet.classifier[0] = nn.Dropout(p=0.5)\n",
        "mobilenet.classifier[1] = nn.Linear(mobilenet.classifier[1].in_features, 5)\n",
        "\n",
        "# Transfer the model to the GPU if it is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "mobilenet = mobilenet.to(device)\n",
        "\n",
        "# Loss Function and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(mobilenet.parameters(), lr=1e-4)\n",
        "\n",
        "# Learning rate decay during training\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSrNl-oshadA",
        "outputId": "2e11f041-0c43-4ac2-b5a9-ca12053af4ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13.6M/13.6M [00:00<00:00, 125MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    mobilenet.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = mobilenet(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_acc = 100 * correct / total\n",
        "\n",
        "    val_loss, val_acc = evaluate(mobilenet, val_loader)\n",
        "    test_loss, test_acc = evaluate(mobilenet, test_loader)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "    print(f\"  Train     => Loss: {train_loss:.4f}, Accuracy: {train_acc:.2f}%\")\n",
        "    print(f\"  Validation=> Loss: {val_loss:.4f}, Accuracy: {val_acc:.2f}%\")\n",
        "    print(f\"  Test      => Loss: {test_loss:.4f}, Accuracy: {test_acc:.2f}%\")\n",
        "    print(\"-\" * 60)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrGTlVgThaao",
        "outputId": "be37eaee-324f-4330-918c-3e67059a898e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20]\n",
            "  Train     => Loss: 0.7976, Accuracy: 73.41%\n",
            "  Validation=> Loss: 0.6947, Accuracy: 76.14%\n",
            "  Test      => Loss: 0.6965, Accuracy: 76.74%\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "param_grid = {\n",
        "    'batch_size': [16, 32],\n",
        "    'lr': [1e-3, 1e-4],\n",
        "    'dropout': [0.3, 0.5]\n",
        "}\n",
        "\n",
        "grid = list(ParameterGrid(param_grid))\n"
      ],
      "metadata": {
        "id": "FgHKiAAmhaY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-gradcam"
      ],
      "metadata": {
        "id": "5tgVvnTrhaUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_m-431WdkDw"
      },
      "outputs": [],
      "source": [
        "pip install captum"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision captum matplotlib numpy\n",
        "\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from captum.attr import IntegratedGradients\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
        "model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, 5)\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_dataset = datasets.ImageFolder(\n",
        "    '/content/retinopathy_split/test',\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "def show_image(img_tensor):\n",
        "    img = img_tensor.cpu().numpy().transpose(1, 2, 0)\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    img = std * img + mean\n",
        "    img = np.clip(img, 0, 1)\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "def visualize_attr(attr):\n",
        "    attr = attr.cpu().detach().numpy()\n",
        "\n",
        "    if attr.ndim == 4:\n",
        "        attr = attr.squeeze(0)\n",
        "    if attr.ndim == 3 and attr.shape[0] == 1:\n",
        "        attr = attr.squeeze(0)\n",
        "    elif attr.ndim == 3 and attr.shape[0] == 3:\n",
        "        attr = attr.transpose(1, 2, 0)\n",
        "        attr = np.sum(np.abs(attr), axis=2)\n",
        "    elif attr.ndim == 2:\n",
        "        pass\n",
        "    else:\n",
        "        raise ValueError(f\"Ø´ÙƒÙ„ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {attr.shape}\")\n",
        "\n",
        "    attr = (attr - attr.min()) / (attr.max() - attr.min() + 1e-8)\n",
        "\n",
        "    plt.imshow(attr, cmap='hot')\n",
        "    plt.colorbar()\n",
        "    plt.axis('off')\n",
        "    plt.title('Importance Map')\n",
        "    plt.show()\n",
        "\n",
        "idx = random.randint(0, len(test_dataset)-1)\n",
        "image, label = test_dataset[idx]\n",
        "image = image.to(device)\n",
        "label = torch.tensor(label).to(device)\n",
        "\n",
        "ig = IntegratedGradients(model)\n",
        "attributions, delta = ig.attribute(\n",
        "    inputs=image.unsqueeze(0),\n",
        "    target=label,\n",
        "    return_convergence_delta=True\n",
        ")\n",
        "\n",
        "print(f\"Convergence Delta: {delta.item():.4f}\")\n",
        "show_image(image)\n",
        "visualize_attr(attributions)\n"
      ],
      "metadata": {
        "id": "4dLRk7p2iFKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ø«Ø§Ù†ÙŠØ§Ù‹: Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from captum.attr import IntegratedGradients\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Ø«Ø§Ù„Ø«Ø§Ù‹: ØªØ¹Ø±ÙŠÙ Ø§Ù„Ø¬Ù‡Ø§Ø² (GPU Ø¥Ø°Ø§ Ù…ØªÙˆÙØ±)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Ø±Ø§Ø¨Ø¹Ø§Ù‹: ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ (ÙŠØ¬Ø¨ Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ù‡Ø°Ø§ Ø§Ù„Ø¬Ø²Ø¡ Ø¨Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø§Ù„Ø®Ø§Øµ)\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
        "model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, 5)  # ØªØ¹Ø¯ÙŠÙ„ Ù„ 5 ÙØ¦Ø§Øª\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Ø®Ø§Ù…Ø³Ø§Ù‹: Ø¥Ø¹Ø¯Ø§Ø¯ ØªØ­ÙˆÙŠÙ„Ø§Øª Ø§Ù„ØµÙˆØ±\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Ø³Ø§Ø¯Ø³Ø§Ù‹: ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±\n",
        "test_dataset = datasets.ImageFolder(\n",
        "    '/content/retinopathy_split/test',  # ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù…Ø³Ø§Ø± Ø­Ø³Ø¨ Ø¨ÙŠØ§Ù†Ø§ØªÙƒ\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "# Ø³Ø§Ø¨Ø¹Ø§Ù‹: Ø¯Ø§Ù„Ø© Ù„Ø¹Ø±Ø¶ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©\n",
        "def show_image(img_tensor):\n",
        "    img = img_tensor.cpu().numpy().transpose(1, 2, 0)\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    img = std * img + mean  # Ø¹ÙƒØ³ Ø§Ù„ØªØ·Ø¨ÙŠØ¹\n",
        "    img = np.clip(img, 0, 1)\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Ø«Ø§Ù…Ù†Ø§Ù‹: Ø¯Ø§Ù„Ø© Ù…Ø±Ù†Ø© Ù„Ø¹Ø±Ø¶ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø£Ù‡Ù…ÙŠØ©\n",
        "def visualize_attr(attr):\n",
        "    attr = attr.cpu().detach().numpy()\n",
        "\n",
        "    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©\n",
        "    if attr.ndim == 4:\n",
        "        attr = attr.squeeze(0)\n",
        "    if attr.ndim == 3 and attr.shape[0] == 1:\n",
        "        attr = attr.squeeze(0)\n",
        "    elif attr.ndim == 3 and attr.shape[0] == 3:\n",
        "        attr = attr.transpose(1, 2, 0)\n",
        "        attr = np.sum(np.abs(attr), axis=2)\n",
        "    elif attr.ndim == 2:\n",
        "        pass\n",
        "    else:\n",
        "        raise ValueError(f\"Ø´ÙƒÙ„ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {attr.shape}\")\n",
        "\n",
        "    # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù‚ÙŠÙ…\n",
        "    attr = (attr - attr.min()) / (attr.max() - attr.min() + 1e-8)\n",
        "\n",
        "    # Ø§Ù„Ø¹Ø±Ø¶\n",
        "    plt.imshow(attr, cmap='hot')\n",
        "    plt.colorbar()\n",
        "    plt.axis('off')\n",
        "    plt.title('Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø£Ù‡Ù…ÙŠØ©')\n",
        "    plt.show()\n",
        "\n",
        "# ØªØ§Ø³Ø¹Ø§Ù‹: Ø§Ø®ØªÙŠØ§Ø± Ø¹ÙŠÙ†Ø© Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
        "idx = random.randint(0, len(test_dataset)-1)\n",
        "image, label = test_dataset[idx]\n",
        "image = image.to(device)\n",
        "label = torch.tensor(label).to(device)\n",
        "\n",
        "# Ø¹Ø§Ø´Ø±Ø§Ù‹: Ø­Ø³Ø§Ø¨ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø£Ù‡Ù…ÙŠØ©\n",
        "ig = IntegratedGradients(model)\n",
        "attributions, delta = ig.attribute(\n",
        "    inputs=image.unsqueeze(0),\n",
        "    target=label,\n",
        "    return_convergence_delta=True\n",
        ")\n",
        "\n",
        "# Ø§Ù„Ø­Ø§Ø¯ÙŠ Ø¹Ø´Ø±: Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
        "print(f\"Convergence Delta: {delta.item():.4f}\")\n",
        "show_image(image)\n",
        "visualize_attr(attributions)\n"
      ],
      "metadata": {
        "id": "6l9yCq1wiFG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³\n",
        "    print(classification_report(all_labels, all_preds, target_names=class_names, digits=4))\n",
        "\n",
        "    # Ø·Ø¨Ø§Ø¹Ø© confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "# Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ø¯Ø§Ù„Ø© Ø¹Ù„Ù‰ test_loader\n",
        "evaluate_model(mobilenet, test_loader)"
      ],
      "metadata": {
        "id": "YWJMxDVwR5ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://drive.google.com/file/d/1LD6v33J7z5Vt4CnVOAs4kWICHIt5JLyg/view?usp=sharing"
      ],
      "metadata": {
        "id": "n0gwdTC9X0IB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}